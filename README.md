## ğŸš€ SurviverFlow â€“ End-to-End MLOps Pipeline for Survival Prediction

[![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)](https://www.python.org/)
[![Flask](https://img.shields.io/badge/Flask-Web_App-lightgrey?logo=flask)](https://flask.palletsprojects.com/)
[![Redis](https://img.shields.io/badge/Redis-Upstash-red?logo=redis)](https://upstash.com/)
[![Airflow](https://img.shields.io/badge/Airflow-Orchestration-blue?logo=apacheairflow)](https://airflow.apache.org/)
[![Prometheus](https://img.shields.io/badge/Monitoring-Prometheus-orange?logo=prometheus)](https://prometheus.io/)
[![Grafana](https://img.shields.io/badge/Grafana-Dashboard-orange?logo=grafana)](https://grafana.com/)
[![Blog](https://img.shields.io/badge/Read-Blog-blue?logo=medium)](https://medium.com/@dsdineshnitrr/building-surviverflow-an-end-to-end-mlops-pipeline-with-redis-airflow-prometheus-render-b2f3cb341c14)


**SurviverFlow** is a scalable, production-ready MLOps pipeline that predicts passenger survival using the survival prediction dataset. It covers the entire ML lifecycleâ€”**data ingestion**, **feature storage**, **model training**, **drift detection**, **deployment**, and **monitoring**â€”with modern tools like Redis, Airflow, Flask, Prometheus, and Render.

---

## ğŸ“š Table of Contents

- [ğŸ”‘ Key Features](#-key-features)
- [ğŸ§± Architecture Overview](#-architecture-overview)
- [ğŸ“Š MLOps Workflow Diagram](#-mlops-workflow-diagram)
- [ğŸ—‚ï¸ Project Structure](#ï¸-project-structure)
- [âš™ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸ”„ MLOps Pipeline Breakdown](#-mlops-pipeline-breakdown)
- [ğŸ–¼ï¸ Live Web App Interface](#ï¸-live-web-app-interface)
- [ğŸ“Š Monitoring via Grafana](#-monitoring-via-grafana)
- [ğŸ”§ How to Run Locally](#-how-to-run-locally)
- [ğŸŒ Deployment (Render)](#-deployment-render)
- [ğŸ“ˆ Monitoring Metrics](#-monitoring-metrics)
- [âœ… Live App](#-live-app)
- [ğŸ“Œ Lessons & Enhancements](#-lessons--enhancements)
- [ğŸ™Œ Acknowledgements](#-acknowledgements)
- [ğŸ“„ License](#-license)

---

## ğŸ”‘ Key Features

- âœ… Automated Data Ingestion (GCP â†’ PostgreSQL via Airflow)
- âš™ï¸ Redis Feature Store with real-time + batch support
- ğŸ§  Random Forest Classifier with hyperparameter tuning
- ğŸ§¼ Feature Engineering + Class Balancing (SMOTE)
- ğŸŒ Flask Web App for Real-Time Inference
- ğŸ§  Drift Detection via Alibi-Detect (KSDrift)
- ğŸ“ˆ Monitoring via Prometheus + Grafana
- ğŸ³ Dockerized & CI/CD-ready (Render)

---

## ğŸ§± Architecture Overview

```mermaid
graph TD
  A[GCS Bucket] -->|CSV| B[Airflow DAG]
  B --> C[PostgreSQL]
  C --> D[Data Ingestion Script]
  D --> E[Train/Test CSVs]
  E --> F[Data Processing & Feature Engineering]
  F --> G[Redis Feature Store]
  G --> H[Model Training]
  H --> I[Model.pkl]
  I --> J[Flask API]
  G --> J
  J -->|/predict| UserInput[User Form]
  J --> K[Drift Detection - Alibi]
  J --> L[Prometheus /metrics]
  L --> M[Grafana Dashboard]
```

---
## ğŸ“Š MLOps Workflow Diagram

```mermaid
graph TD

  subgraph Setup
    A[Database Setup]
    B[Project Setup]
  end

  subgraph Pipeline
    C[ETL Pipeline - Airflow]
    D[Data Ingestion - GCS to PostgreSQL]
    E[Jupyter Notebook Testing]
    F[Feature Store Setup - Redis]
    G[Data Processing and Feature Storing]
    H[Model Training from Redis]
    I[Training Pipeline]
  end

  subgraph Deployment_and_Monitoring
    J[Data and Code Versioning]
    K[User App - Flask]
    K1[Deployment on Render]
    L[Data Drift Detection - Alibi]
    M[ML Monitoring - Prometheus and Grafana]
  end

  A --> B --> C --> D --> E --> F --> G --> H --> I --> J --> K --> K1 --> L --> M

```

## ğŸ—‚ï¸ Project Structure

```
SURVIVERFLOW-main
â”œâ”€â”€ app.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ .astro/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ extract_data_from_gcp.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ feature_store.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ pipeline/
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ models/random_forest_model.pkl
â”œâ”€â”€ config/
â”œâ”€â”€ notebook/
â”œâ”€â”€ prometheus.yml
â”œâ”€â”€ render.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py
```

---

## âš™ï¸ Tech Stack

| Layer           | Tools Used                              |
|----------------|------------------------------------------|
| Data Source     | GCP Bucket, PostgreSQL                   |
| Workflow Engine | Apache Airflow (Astro CLI)               |
| Feature Store   | Redis (Local Docker + Upstash Cloud)     |
| Model Training  | scikit-learn, Pandas, SMOTE              |
| Drift Detection | Alibi-Detect (KSDrift)                   |
| Monitoring      | Prometheus, Grafana                      |
| Serving Layer   | Flask + HTML                             |
| Deployment      | Docker, Render                           |

---

## ğŸ”„ MLOps Pipeline Breakdown

### ğŸ§® Step 1: Data Ingestion
- Load CSV from **GCP Bucket**
- Airflow DAG writes data into **PostgreSQL**
- Validates schema and handles **null values**

### ğŸ—ƒï¸ Step 2: Feature Store with Redis
- Store extracted features in **Redis**
- Supports both **batch and real-time** access
- Dual-mode support: **Local Docker** & **Upstash (Cloud Redis)**

### ğŸ§¼ Step 3: Data Preprocessing
- Handle **missing values** (Age, Fare, Embarked)
- Perform **Label Encoding** and **Feature Engineering**
- Balance classes using **SMOTE**

### ğŸ§  Step 4: Model Training
- Data is fetched directly from **Redis**
- Trains a **RandomForestClassifier** using **RandomizedSearchCV**
- Saves the model as `.pkl`

### ğŸ”® Step 5: Real-Time Prediction + Drift Detection
- Flask app exposes `/predict` route for **real-time inference**
- **Alibi Detect KSDrift** checks for data distribution shift
- **Prometheus** tracks prediction and drift metrics

### ğŸ“Š Step 6: Monitoring
- **Prometheus** exposes `/metrics` endpoint
- **Grafana Dashboards** visualize system usage and drift trends

### ğŸš€ Step 7: Deployment
- Fully containerized using **Docker + Gunicorn**
- **Render** used for one-click deployment
- Secrets like `REDIS_URL` managed securely via **Render Dashboard**

---

## ğŸ–¼ï¸ Live Web App Interface

### âœ… Likely to Survive  
![Survive](https://raw.githubusercontent.com/aimldinesh/SURVIVERFLOW/main/Images/Prediction_output/Survive.PNG)

### âŒ Likely to Not Survive  
![Not Survive](https://raw.githubusercontent.com/aimldinesh/SURVIVERFLOW/main/Images/Prediction_output/Not_Survive.PNG)

---

## ğŸ“Š Monitoring via Grafana

![Drift Count](https://raw.githubusercontent.com/aimldinesh/SURVIVERFLOW/main/Images/Grafana/grafana_drift_count.PNG)
![Prediction Count](https://raw.githubusercontent.com/aimldinesh/SURVIVERFLOW/main/Images/Grafana/grafana_prediction_count.PNG)
![Drift Graph](https://raw.githubusercontent.com/aimldinesh/SURVIVERFLOW/main/Images/Grafana/grafana_drift_count_graph_2.PNG)

---

## ğŸ”§ How to Run Locally

```bash
# 1. Clone the Repo
git clone https://github.com/aimldinesh/SURVIVERFLOW.git
cd SURVIVERFLOW

# 2. Set Redis URL in .env
echo "REDIS_URL=your_upstash_redis_url" > .env

# 3. Run Flask App Locally
python app.py

# OR Build with Docker
docker build -t survivorflow-app .
docker run -p 5000:5000 survivorflow-app
```

---

## ğŸŒ Deployment (Render)

Render setup with Docker + Redis service:

```yaml
services:
  - type: web
    name: survivorflow-app
    env: docker
    dockerfilePath: ./Dockerfile
    envVars:
      - key: REDIS_URL
        sync: false  # Set manually in Render dashboard
```

---

## ğŸ“ˆ Monitoring Metrics

| Metric            | Description                        |
|-------------------|------------------------------------|
| `prediction_count`| Number of predictions made         |
| `drift_count`     | Number of drift detections         |

Access at `/metrics` endpoint.

---

## âœ… Live App

ğŸ‘‰ [https://surviverflow-1.onrender.com](https://surviverflow-1.onrender.com)

---

## ğŸ“Œ Lessons & Enhancements

### ğŸ” Key Learnings
- Using **Redis as a Feature Store** helped prevent data leakage
- **Drift Monitoring** with Alibi ensures long-term model reliability
- **Docker + Render** enabled fast and reproducible CI/CD deployment

ğŸ“Œ Want to learn how this project was built?
- ğŸ“ I wrote a detailed Medium blog covering the full architecture, tools, and deployment process.
ğŸ‘‰ [Read the blog on Medium](https://medium.com/@dsdineshnitrr/building-surviverflow-an-end-to-end-mlops-pipeline-with-redis-airflow-prometheus-render-b2f3cb341c14)

### ğŸ› ï¸ Future Enhancements
- âœ… CI/CD via **GitHub Actions**
- âœ… Implement **Feature Versioning**
- âœ… Add **Retraining Pipelines**
- âœ… Set up **Slack/Email Alerts** for drift or failure
- âœ… Integrate **MLflow** for model registry and tracking

---

## ğŸ™Œ Acknowledgements

- [Titanic Dataset â€“ Kaggle](https://www.kaggle.com/c/titanic)
- [Render](https://render.com)
- [Upstash Redis](https://upstash.com/)
- [Alibi-Detect](https://docs.seldon.io/projects/alibi-detect/)
- [ChatGPT](https://openai.com/chatgpt) â€“ for architectural guidance

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.
